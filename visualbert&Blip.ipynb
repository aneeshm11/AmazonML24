{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9377434,"sourceType":"datasetVersion","datasetId":5688358}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom transformers import VisualBertModel, VisualBertConfig, BertTokenizer, BlipProcessor, BlipForConditionalGeneration\nfrom torchvision import transforms, models\nfrom torchvision.models import resnet50\nfrom PIL import Image\nfrom io import BytesIO\nimport requests\nimport pandas as pd\nfrom tqdm import tqdm\n","metadata":{"vscode":{"languageId":"plaintext"},"execution":{"iopub.status.busy":"2024-09-15T03:55:36.329804Z","iopub.execute_input":"2024-09-15T03:55:36.330268Z","iopub.status.idle":"2024-09-15T03:55:36.336699Z","shell.execute_reply.started":"2024-09-15T03:55:36.330226Z","shell.execute_reply":"2024-09-15T03:55:36.335725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nconversion_to_tons = {'gram': 1e-6,'ton': 1,'kilogram': 0.001,'ounce': 2.8349523125e-5,'pound': 0.00045359237,'carat': 2e-7,'microgram': 1e-12,'milligram': 1e-9 }\n\ndef convert_to_tons(row):\n    try:\n        value, unit = row['entity_value'].lower().split(' ', 1)\n        value = float(value)\n        conversion_factor = conversion_to_tons.get(unit.strip(), 1)\n        return value * conversion_factor\n    except ValueError:\n        return None  \n\nweight_units = ['gram', 'ton', 'kilogram', 'ounce', 'pound', 'carat', 'microgram', 'milligram']\n\ntrain_df = pd.read_csv('/kaggle/input/pedalanja/student_resource 3/dataset/train.csv')\nentity_name = 'item_weight'\nentity_df = train_df[train_df['entity_name'] == entity_name].copy()\n\nentity_df = entity_df.dropna(subset=['entity_value'])\nentity_df = entity_df[~entity_df['entity_value'].str.contains(r'\\[|\\bto\\b|\\be\\+17\\b', regex=True, na=False)]\nentity_df = entity_df[entity_df['entity_value'].str.contains('|'.join(weight_units), case=False, na=False)]\n\nentity_df['entity_value'] = entity_df.apply(convert_to_tons, axis=1)\nentity_df = entity_df[(entity_df['entity_value'] > 5e-8) & (entity_df['entity_value'] < 1)]\nentity_df = entity_df.dropna(subset=['entity_value'])\nentity_df = entity_df.head(500)\n\nblip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\nblip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n","metadata":{"vscode":{"languageId":"plaintext"},"execution":{"iopub.status.busy":"2024-09-15T03:55:36.870001Z","iopub.execute_input":"2024-09-15T03:55:36.871093Z","iopub.status.idle":"2024-09-15T03:55:40.255222Z","shell.execute_reply.started":"2024-09-15T03:55:36.871044Z","shell.execute_reply":"2024-09-15T03:55:40.254042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass AdaptiveResNet(nn.Module):\n    def __init__(self):\n        super(AdaptiveResNet, self).__init__()\n        resnet = resnet50(pretrained=True)\n        self.features = nn.Sequential(*list(resnet.children())[:-2])  \n        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.adaptive_pool(x)\n        return x.view(x.size(0), -1)\n\nadaptive_resnet = AdaptiveResNet()\nadaptive_resnet.eval()\n\ndef generate_caption(image_url):\n    response = requests.get(image_url)\n    image = Image.open(BytesIO(response.content)).convert('RGB')\n    inputs = blip_processor(images=image, return_tensors=\"pt\")\n    with torch.no_grad():\n        output = blip_model.generate(**inputs, max_new_tokens=50)\n    caption = blip_processor.decode(output[0], skip_special_tokens=True)\n    words = caption.split()\n    cleaned_words = [words[i] for i in range(len(words)) if i == 0 or words[i] != words[i-1]]\n    return ' '.join(cleaned_words)\n\ndef extract_visual_features(image_url):\n    response = requests.get(image_url)\n    image = Image.open(BytesIO(response.content)).convert('RGB')\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    image = transform(image).unsqueeze(0)\n    with torch.no_grad():\n        features = adaptive_resnet(image)\n    return features  # Shape: (1, 2048)\n\nclass ItemVolumeDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length=64):\n        self.dataframe = dataframe\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image_url = row['image_link']\n        volume = row['entity_value']\n\n        caption = generate_caption(image_url)\n        inputs = self.tokenizer(caption, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n        \n        visual_embeds = extract_visual_features(image_url)\n        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n\n        return {\n            'input_ids': inputs['input_ids'].squeeze(0),\n            'attention_mask': inputs['attention_mask'].squeeze(0),\n            'visual_embeds': visual_embeds.squeeze(0),\n            'visual_attention_mask': visual_attention_mask.squeeze(0),\n            'volume': torch.tensor(volume, dtype=torch.float)\n        }\n\nclass EntityExtractionModel(nn.Module):\n    def __init__(self, hidden_size=768):\n        super(EntityExtractionModel, self).__init__()\n        self.config = VisualBertConfig.from_pretrained('uclanlp/visualbert-vqa-coco-pre')\n        self.visualbert = VisualBertModel.from_pretrained('uclanlp/visualbert-vqa-coco-pre', config=self.config)\n        \n        self.regression_head = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 1)\n        )\n\n    def forward(self, input_ids, attention_mask, visual_embeds, visual_attention_mask):\n        batch_size = input_ids.size(0)\n        visual_embeds = visual_embeds.expand(batch_size, -1, -1)\n        visual_attention_mask = visual_attention_mask.expand(batch_size, -1)\n\n        outputs = self.visualbert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            visual_embeds=visual_embeds,\n            visual_attention_mask=visual_attention_mask\n        )\n        pooled_output = outputs.pooler_output\n        numeric_value = self.regression_head(pooled_output)\n        return numeric_value.squeeze(-1)\n","metadata":{"vscode":{"languageId":"plaintext"},"execution":{"iopub.status.busy":"2024-09-15T03:55:40.257606Z","iopub.execute_input":"2024-09-15T03:55:40.258193Z","iopub.status.idle":"2024-09-15T03:55:40.733016Z","shell.execute_reply.started":"2024-09-15T03:55:40.258139Z","shell.execute_reply":"2024-09-15T03:55:40.732169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = EntityExtractionModel()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\ndataset = ItemVolumeDataset(entity_df, tokenizer)\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n\nlearning_rate = 1e-4\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nscheduler = ReduceLROnPlateau(optimizer, patience=2, factor=0.1)\n\ncriterion = nn.MSELoss()\n\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    \n    for batch in tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch'):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        visual_embeds = batch['visual_embeds'].to(device)\n        visual_attention_mask = batch['visual_attention_mask'].to(device)\n        volume = batch['volume'].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            visual_embeds=visual_embeds,\n            visual_attention_mask=visual_attention_mask\n        )\n        loss = criterion(outputs, volume)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    \n    avg_loss = total_loss / len(dataloader)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n    \n    scheduler.step(avg_loss)\n\ntorch.save(model.state_dict(), 'visual_bert_volume_model.pth')","metadata":{"vscode":{"languageId":"plaintext"},"execution":{"iopub.status.busy":"2024-09-15T03:59:26.113363Z","iopub.execute_input":"2024-09-15T03:59:26.113787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}